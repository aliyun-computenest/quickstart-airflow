{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Airflow\u8ba1\u7b97\u5de2\u5feb\u901f\u90e8\u7f72 \u514d\u8d23\u58f0\u660e\uff1a \u672c\u670d\u52a1\u7531\u7b2c\u4e09\u65b9\u63d0\u4f9b\uff0c\u6211\u4eec\u5c3d\u529b\u786e\u4fdd\u5176\u5b89\u5168\u6027\u3001\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u5176\u5b8c\u5168\u514d\u4e8e\u6545\u969c\u3001\u4e2d\u65ad\u3001\u9519\u8bef\u6216\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u672c\u516c\u53f8\u5728\u6b64\u58f0\u660e\uff1a\u5bf9\u4e8e\u672c\u670d\u52a1\u7684\u5185\u5bb9\u3001\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u3001\u9002\u7528\u6027\u4ee5\u53ca\u53ca\u65f6\u6027\u4e0d\u4f5c\u4efb\u4f55\u9648\u8ff0\u3001\u4fdd\u8bc1\u6216\u627f\u8bfa\uff0c\u4e0d\u5bf9\u60a8\u4f7f\u7528\u672c\u670d\u52a1\u6240\u4ea7\u751f\u7684\u4efb\u4f55\u76f4\u63a5\u6216\u95f4\u63a5\u7684\u635f\u5931\u6216\u635f\u5bb3\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\uff1b\u5bf9\u4e8e\u60a8\u901a\u8fc7\u672c\u670d\u52a1\u8bbf\u95ee\u7684\u7b2c\u4e09\u65b9\u7f51\u7ad9\u3001\u5e94\u7528\u7a0b\u5e8f\u3001\u4ea7\u54c1\u548c\u670d\u52a1\uff0c\u4e0d\u5bf9\u5176\u5185\u5bb9\u3001\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u3001\u9002\u7528\u6027\u4ee5\u53ca\u53ca\u65f6\u6027\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\uff0c\u60a8\u5e94\u81ea\u884c\u627f\u62c5\u4f7f\u7528\u540e\u679c\u4ea7\u751f\u7684\u98ce\u9669\u548c\u8d23\u4efb\uff1b\u5bf9\u4e8e\u56e0\u60a8\u4f7f\u7528\u672c\u670d\u52a1\u800c\u4ea7\u751f\u7684\u4efb\u4f55\u635f\u5931\u3001\u635f\u5bb3\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u76f4\u63a5\u635f\u5931\u3001\u95f4\u63a5\u635f\u5931\u3001\u5229\u6da6\u635f\u5931\u3001\u5546\u8a89\u635f\u5931\u3001\u6570\u636e\u635f\u5931\u6216\u5176\u4ed6\u7ecf\u6d4e\u635f\u5931\uff0c\u4e0d\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\uff0c\u5373\u4f7f\u672c\u516c\u53f8\u4e8b\u5148\u5df2\u88ab\u544a\u77e5\u53ef\u80fd\u5b58\u5728\u6b64\u7c7b\u635f\u5931\u6216\u635f\u5bb3\u7684\u53ef\u80fd\u6027\uff1b\u6211\u4eec\u4fdd\u7559\u4e0d\u65f6\u4fee\u6539\u672c\u58f0\u660e\u7684\u6743\u5229\uff0c\u56e0\u6b64\u8bf7\u60a8\u5728\u4f7f\u7528\u672c\u670d\u52a1\u524d\u5b9a\u671f\u68c0\u67e5\u672c\u58f0\u660e\u3002\u5982\u679c\u60a8\u5bf9\u672c\u58f0\u660e\u6216\u672c\u670d\u52a1\u5b58\u5728\u4efb\u4f55\u95ee\u9898\u6216\u7591\u95ee\uff0c\u8bf7\u8054\u7cfb\u6211\u4eec\u3002 \u6982\u8ff0 Apache Airflow \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u5e73\u53f0\uff0c\u7528\u4e8e\u7f16\u5199\u3001\u8c03\u5ea6\u548c\u76d1\u63a7\u5de5\u4f5c\u6d41\uff08Workflows\uff09\u3002\u5b83\u6700\u521d\u7531 Airbnb \u5f00\u53d1\uff0c\u5e76\u4e8e 2016 \u5e74\u6350\u8d60\u7ed9 Apache \u8f6f\u4ef6\u57fa\u91d1\u4f1a\u3002Airflow \u7684\u6838\u5fc3\u7406\u5ff5\u662f\u901a\u8fc7\u4ee3\u7801\u6765\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u4f7f\u5f97\u5de5\u4f5c\u6d41\u7684\u7ba1\u7406\u548c\u7ef4\u62a4\u66f4\u52a0\u7075\u6d3b\u548c\u53ef\u6269\u5c55, github\u793e\u533a\u5730\u5740\u89c1 \u94fe\u63a5 \u3002 \u8ba1\u8d39\u8bf4\u660e \u90e8\u7f72Airflow\u7684\u8d39\u7528\u4e3b\u8981\u6d89\u53ca\uff1a \u6240\u9009vCPU\u4e0e\u5185\u5b58\u89c4\u683c \u7cfb\u7edf\u76d8\u7c7b\u578b\u53ca\u5bb9\u91cf \u516c\u7f51\u5e26\u5bbd Kubernetes\u96c6\u7fa4\u89c4\u683c \u90e8\u7f72\u67b6\u6784 RAM\u8d26\u53f7\u6240\u9700\u6743\u9650 \u90e8\u7f72Airflow\uff0c\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002 \u8bf4\u660e \uff1a\u5f53\u60a8\u7684\u8d26\u53f7\u662fRAM\u8d26\u53f7\u65f6\uff0c\u624d\u9700\u8981\u6dfb\u52a0\u6b64\u6743\u9650\u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 AliyunPostGreSQLFullAccess \u7ba1\u7406\u4e91\u6570\u636e\u5e93\u670d\u52a1\uff08PostGreSQL\uff09\u7684\u6743\u9650 AliyunSLBFullAccess \u7ba1\u7406\u8d1f\u8f7d\u5747\u8861\uff08SLB\uff09\u7684\u6743\u9650 \u90e8\u7f72\u6d41\u7a0b 1.\u8bbf\u95eeAirflow\u670d\u52a1 \u90e8\u7f72\u94fe\u63a5 \uff0c\u6309\u63d0\u793a\u586b\u5199\u90e8\u7f72\u53c2\u6570, \u8fd9\u91cc\u5148\u8fdb\u884c\u5bb9\u5668\u96c6\u7fa4\u8bbe\u7f6e\uff0c\u53ef\u4ee5\u9009\u62e9\u521b\u5efa\u5bb9\u5668\u96c6\u7fa4\uff0c\u4e5f\u652f\u6301\u5df2\u6709\u96c6\u7fa4\u90e8\u7f72\uff0c\u8fd9\u91cc\u65b0\u5efaACK\u96c6\u7fa4\u90e8\u7f72, \u586b\u5165\u65b0\u5efaACK\u96c6\u7fa4\u6240\u9700\u7684\u53c2\u6570\uff1a 2.\u7136\u540e\u8bbe\u7f6e\u5b58\u653eDAG\u6587\u4ef6\u7684Git\u4ed3\u5e93\u914d\u7f6e\uff0c\u672c\u670d\u52a1\u91c7\u7528Git\u4ed3\u5e93\u4fdd\u5b58DAG\u6587\u4ef6\u7684\u65b9\u5f0f\uff0c\u8fd9\u91cc\u652f\u6301\u516c\u5f00\u548c\u79c1\u6709 \u4e24\u79cd\u4ed3\u5e93\uff0c\u9009\u62e9\u79c1\u6709\u4ed3\u5e93\u65f6\uff0c\u9700\u8981\u751f\u6210accessToken\u5f53\u4f5c\u8bbf\u95ee\u51ed\u8bc1\u3002 3.\u8bbe\u7f6eairflow web\u767b\u5f55\u7684\u8d26\u53f7\u548c\u5bc6\u7801\u3002 4.\u8bbe\u7f6eACK\u96c6\u7fa4\u90e8\u7f72\u7684\u7f51\u7edc\u8bbe\u7f6e\uff0c\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u5df2\u6709\u7684Vpc\u548cVSwitch\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u65b0\u5efaVpc\u548cVSwitch\u3002 5.\u786e\u8ba4\u8ba2\u5355\u5b8c\u6210\u540e\u540c\u610f\u670d\u52a1\u534f\u8bae\u5e76\u70b9\u51fb \u7acb\u5373\u521b\u5efa \u8fdb\u5165\u90e8\u7f72\u9636\u6bb5\u3002 6.\u5728\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\u4e2d\u53ef\u4ee5\u770b\u5230\u670d\u52a1\u5b9e\u4f8b\u5177\u4f53\u90e8\u7f72\u8fdb\u5ea6\u3002 7.\u90e8\u7f72\u5b8c\u6210\u540e\u5728\u63a7\u5236\u53f0\u627e\u5230Airflow\u670d\u52a1\u94fe\u63a5\u5e76\u8bbf\u95ee, \u521d\u59cb\u8d26\u53f7\u3001\u5bc6\u7801\u4e3a\u521b\u5efa\u670d\u52a1\u5b9e\u4f8b\u8bbe\u7f6e\u7684\u503c\u3002 \u4f7f\u7528\u65b9\u5f0f \u4e0a\u9762\u670d\u52a1\u5b9e\u4f8b\u5df2\u7ecf\u90e8\u7f72\u5b8c\u6210\uff0c\u90a3\u4e48\u600e\u4e48\u8fd0\u884c\u6211\u4eec\u5b9a\u4e49\u597d\u7684DAG\u5de5\u4f5c\u6d41\u5462\uff0c\u8fd9\u91cc\u4e3b\u8981\u4f9d\u8d56git\u4ed3\u5e93\u53bb\u505a\u540c\u6b65\uff0c \u6211\u4eec\u53ef\u4ee5\u628a\u5199\u597d\u7684DAG\u6587\u4ef6\u63d0\u4ea4\u5230\u521b\u5efa\u670d\u52a1\u5b9e\u4f8b\u65f6\u6240\u51c6\u5907\u7684git\u4ed3\u5e93\u4e2d\uff0c\u7136\u540eairflow-scheduler\u7ec4\u4ef6\u4f1a\u8fdb\u884c\u540c\u6b65\uff0c web\u4e0a\u5c31\u80fd\u770b\u5230\u6211\u4eec\u5b9a\u4e49\u597d\u7684DAG\u6587\u4ef6\uff0c\u7136\u540e\u70b9\u51fbrun\u6309\u94ae\u5c31\u53ef\u4ee5\u8fd0\u884cDAG\u6587\u4ef6\u4e86\u3002 \u4e0b\u9762\u4ee5\u4e00\u4e2a\u7b80\u5355\u7684DAG\u6587\u4ef6\u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u5728airflow\u4e2d\u8fdb\u884c\u8fd0\u884cDAG\u3002 1.\u5728git\u4ed3\u5e93\u4e2d\u521b\u5efaDAG\u6587\u4ef6\uff0c\u6587\u4ef6\u540d\u4e3a hello_world_dag.py \uff0c\u91cc\u9762\u6709\u4e09\u4e2a\u4efb\u52a1\uff0c\u4f1a\u4f9d\u6b21\u6267\u884c\uff1a - \u6253\u5370\"Hello\" - \u6253\u5370\"World\" - \u4f11\u7720300\u79d2 import time from datetime import timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.utils.dates import days_ago # \u5b9a\u4e49\u9ed8\u8ba4\u53c2\u6570 default_args = { 'owner': 'airflow', # DAG \u7684\u6240\u6709\u8005 'start_date': days_ago(1), # DAG \u7684\u5f00\u59cb\u65f6\u95f4\uff081 \u5929\u524d\uff09 'retries': 1, # \u4efb\u52a1\u5931\u8d25\u65f6\u7684\u91cd\u8bd5\u6b21\u6570 'retry_delay': timedelta(minutes=5), # \u91cd\u8bd5\u95f4\u9694 } # \u5b9a\u4e49 DAG \u5bf9\u8c61 with DAG( dag_id='hello_world_dag', # DAG \u7684\u552f\u4e00\u6807\u8bc6\u7b26 default_args=default_args, # \u4f7f\u7528\u9ed8\u8ba4\u53c2\u6570 schedule_interval='@daily', # \u6bcf\u5929\u8fd0\u884c\u4e00\u6b21 catchup=False, # \u662f\u5426\u8865\u8dd1\u5386\u53f2\u4efb\u52a1 ) as dag: # \u5b9a\u4e49\u7b2c\u4e00\u4e2a\u4efb\u52a1\uff1a\u6253\u5370 \"Hello\" def print_hello(): print(\"Hello\") task_hello = PythonOperator( task_id='print_hello', # \u4efb\u52a1\u7684\u552f\u4e00\u6807\u8bc6\u7b26 python_callable=print_hello, # \u8c03\u7528\u7684 Python \u51fd\u6570 ) # \u5b9a\u4e49\u7b2c\u4e8c\u4e2a\u4efb\u52a1\uff1a\u6253\u5370 \"World\" def print_world(): print(\"World\") task_world = PythonOperator( task_id='print_world', python_callable=print_world, ) # \u5b9a\u4e49\u4e00\u4e2a\u4f11\u7720\u4efb\u52a1 def sleep_task(): print(\"Task is sleeping for 300 seconds...\") time.sleep(300) # \u4f11\u7720 300 \u79d2 print(\"Task woke up!\") sleep_operator = PythonOperator( task_id='sleep_task', python_callable=sleep_task, ) # \u8bbe\u7f6e\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb task_hello >> task_world >> sleep_operator 2.\u63d0\u4ea4DAG\u6587\u4ef6\u5230git\u4ed3\u5e93\u4e2d\uff0c\u7136\u540e\u53bbweb\u7aef\u67e5\u770b\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684DAG\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4f1a\u6709\u5ef6\u65f6\uff0c\u76ee\u524d\u8bbe\u7f6e\u7684\u662f\u6bcf10s\u540c\u6b65\u4e00\u6b21\u3002 3.\u6267\u884c\u8fd9\u4e2aDAG\uff0c\u70b9\u51fbrun\u6309\u94ae\uff0c\u70b9\u51fb\u8fdb\u884c\uff0c\u53ef\u4ee5\u770b\u5230\u6267\u884c\u8bb0\u5f55\uff0c\u70b9\u51fbGraph, \u53ef\u4ee5\u770b\u5230\u5177\u4f53\u6267\u884c\u6b65\u9aa4\uff0c \u53ef\u4ee5\u770b\u5230print_hello\u548cprint_world\u90fd\u5df2\u7ecf\u6267\u884c\u5b8c\u4e86\uff0csleep_task\u8fd8\u5728\u6267\u884c\u4e2d\uff0c\u8fd9\u4e2a\u529f\u80fd\u786e\u5b9e\u5f88\u5f3a\u5927\u3002 4.\u70b9\u51fb\u8fd8\u5728\u6267\u884c\u4e2d\u7684sleep_task\uff0c\u53ef\u4ee5\u5728Logs\u91cc\u770b\u5230\u8f93\u51fa\u4fe1\u606f\uff0c\u91cc\u9762\u8f93\u51fa\u4e86\u4f1asleep 300\u79d2\uff0c\u53ef\u89c1\u5728\u6b63\u5e38\u6267\u884c\u3002 \u901a\u8fc7\u4e0a\u9762\u8fd9\u4e2a\u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u51faairflow\u6574\u4f53\u529f\u80fd\u8fd8\u662f\u5f88\u5f3a\u5927\u7684\uff0c\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230DAG\u7684\u6267\u884c\u60c5\u51b5\uff0c\u5e76\u4e14\u5c06\u6bcf\u4e00\u6b65\u7684 \u6267\u884c\u8fc7\u7a0b\u90fd\u4ee5\u56fe\u5f62\u5316\u7684\u65b9\u5f0f\u663e\u793a\u51fa\u6765\uff0c\u91cc\u9762\u8fd8\u6709\u6267\u884c\u65f6\u95f4\u548c\u65e5\u5fd7\uff0c\u7528\u6765\u505a\u5de5\u4f5c\u6d41\u8fd8\u662f\u5f88\u597d\u7528\u7684\u3002 \u5e38\u89c1\u95ee\u9898\u7b54\u7591 1.git-sync\u5bb9\u5668\u542f\u52a8\u5931\u8d25 \u7531\u4e8e\u9700\u8981\u8bbf\u95eegit\u4ed3\u5e93\uff0c\u56fd\u5185\u673a\u5668\u53ef\u80fd\u65e0\u6cd5\u8bbf\u95eegithub\uff0c\u53ef\u4ee5\u9009\u62e9\u6d77\u5916\u5730\u57df\u8fdb\u884c\u90e8\u7f72\uff0c\u53ef\u4ee5\u81ea\u884c\u5bf9\u90e8\u7f72\u7684helm release\u8fdb\u884c\u5347\u7ea7\u4fee\u6539\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u5f0f\u53ef\u4ee5\u53c2\u8003 \u5b98\u65b9values.yaml\u914d\u7f6e \u3002","title":"Airflow\u8ba1\u7b97\u5de2\u5feb\u901f\u90e8\u7f72"},{"location":"#airflow","text":"\u514d\u8d23\u58f0\u660e\uff1a \u672c\u670d\u52a1\u7531\u7b2c\u4e09\u65b9\u63d0\u4f9b\uff0c\u6211\u4eec\u5c3d\u529b\u786e\u4fdd\u5176\u5b89\u5168\u6027\u3001\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u5176\u5b8c\u5168\u514d\u4e8e\u6545\u969c\u3001\u4e2d\u65ad\u3001\u9519\u8bef\u6216\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u672c\u516c\u53f8\u5728\u6b64\u58f0\u660e\uff1a\u5bf9\u4e8e\u672c\u670d\u52a1\u7684\u5185\u5bb9\u3001\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u3001\u9002\u7528\u6027\u4ee5\u53ca\u53ca\u65f6\u6027\u4e0d\u4f5c\u4efb\u4f55\u9648\u8ff0\u3001\u4fdd\u8bc1\u6216\u627f\u8bfa\uff0c\u4e0d\u5bf9\u60a8\u4f7f\u7528\u672c\u670d\u52a1\u6240\u4ea7\u751f\u7684\u4efb\u4f55\u76f4\u63a5\u6216\u95f4\u63a5\u7684\u635f\u5931\u6216\u635f\u5bb3\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\uff1b\u5bf9\u4e8e\u60a8\u901a\u8fc7\u672c\u670d\u52a1\u8bbf\u95ee\u7684\u7b2c\u4e09\u65b9\u7f51\u7ad9\u3001\u5e94\u7528\u7a0b\u5e8f\u3001\u4ea7\u54c1\u548c\u670d\u52a1\uff0c\u4e0d\u5bf9\u5176\u5185\u5bb9\u3001\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u3001\u9002\u7528\u6027\u4ee5\u53ca\u53ca\u65f6\u6027\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\uff0c\u60a8\u5e94\u81ea\u884c\u627f\u62c5\u4f7f\u7528\u540e\u679c\u4ea7\u751f\u7684\u98ce\u9669\u548c\u8d23\u4efb\uff1b\u5bf9\u4e8e\u56e0\u60a8\u4f7f\u7528\u672c\u670d\u52a1\u800c\u4ea7\u751f\u7684\u4efb\u4f55\u635f\u5931\u3001\u635f\u5bb3\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u76f4\u63a5\u635f\u5931\u3001\u95f4\u63a5\u635f\u5931\u3001\u5229\u6da6\u635f\u5931\u3001\u5546\u8a89\u635f\u5931\u3001\u6570\u636e\u635f\u5931\u6216\u5176\u4ed6\u7ecf\u6d4e\u635f\u5931\uff0c\u4e0d\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\uff0c\u5373\u4f7f\u672c\u516c\u53f8\u4e8b\u5148\u5df2\u88ab\u544a\u77e5\u53ef\u80fd\u5b58\u5728\u6b64\u7c7b\u635f\u5931\u6216\u635f\u5bb3\u7684\u53ef\u80fd\u6027\uff1b\u6211\u4eec\u4fdd\u7559\u4e0d\u65f6\u4fee\u6539\u672c\u58f0\u660e\u7684\u6743\u5229\uff0c\u56e0\u6b64\u8bf7\u60a8\u5728\u4f7f\u7528\u672c\u670d\u52a1\u524d\u5b9a\u671f\u68c0\u67e5\u672c\u58f0\u660e\u3002\u5982\u679c\u60a8\u5bf9\u672c\u58f0\u660e\u6216\u672c\u670d\u52a1\u5b58\u5728\u4efb\u4f55\u95ee\u9898\u6216\u7591\u95ee\uff0c\u8bf7\u8054\u7cfb\u6211\u4eec\u3002","title":"Airflow\u8ba1\u7b97\u5de2\u5feb\u901f\u90e8\u7f72"},{"location":"#_1","text":"Apache Airflow \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u5e73\u53f0\uff0c\u7528\u4e8e\u7f16\u5199\u3001\u8c03\u5ea6\u548c\u76d1\u63a7\u5de5\u4f5c\u6d41\uff08Workflows\uff09\u3002\u5b83\u6700\u521d\u7531 Airbnb \u5f00\u53d1\uff0c\u5e76\u4e8e 2016 \u5e74\u6350\u8d60\u7ed9 Apache \u8f6f\u4ef6\u57fa\u91d1\u4f1a\u3002Airflow \u7684\u6838\u5fc3\u7406\u5ff5\u662f\u901a\u8fc7\u4ee3\u7801\u6765\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u4f7f\u5f97\u5de5\u4f5c\u6d41\u7684\u7ba1\u7406\u548c\u7ef4\u62a4\u66f4\u52a0\u7075\u6d3b\u548c\u53ef\u6269\u5c55, github\u793e\u533a\u5730\u5740\u89c1 \u94fe\u63a5 \u3002","title":"\u6982\u8ff0"},{"location":"#_2","text":"\u90e8\u7f72Airflow\u7684\u8d39\u7528\u4e3b\u8981\u6d89\u53ca\uff1a \u6240\u9009vCPU\u4e0e\u5185\u5b58\u89c4\u683c \u7cfb\u7edf\u76d8\u7c7b\u578b\u53ca\u5bb9\u91cf \u516c\u7f51\u5e26\u5bbd Kubernetes\u96c6\u7fa4\u89c4\u683c","title":"\u8ba1\u8d39\u8bf4\u660e"},{"location":"#_3","text":"","title":"\u90e8\u7f72\u67b6\u6784"},{"location":"#ram","text":"\u90e8\u7f72Airflow\uff0c\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002 \u8bf4\u660e \uff1a\u5f53\u60a8\u7684\u8d26\u53f7\u662fRAM\u8d26\u53f7\u65f6\uff0c\u624d\u9700\u8981\u6dfb\u52a0\u6b64\u6743\u9650\u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 AliyunPostGreSQLFullAccess \u7ba1\u7406\u4e91\u6570\u636e\u5e93\u670d\u52a1\uff08PostGreSQL\uff09\u7684\u6743\u9650 AliyunSLBFullAccess \u7ba1\u7406\u8d1f\u8f7d\u5747\u8861\uff08SLB\uff09\u7684\u6743\u9650","title":"RAM\u8d26\u53f7\u6240\u9700\u6743\u9650"},{"location":"#_4","text":"1.\u8bbf\u95eeAirflow\u670d\u52a1 \u90e8\u7f72\u94fe\u63a5 \uff0c\u6309\u63d0\u793a\u586b\u5199\u90e8\u7f72\u53c2\u6570, \u8fd9\u91cc\u5148\u8fdb\u884c\u5bb9\u5668\u96c6\u7fa4\u8bbe\u7f6e\uff0c\u53ef\u4ee5\u9009\u62e9\u521b\u5efa\u5bb9\u5668\u96c6\u7fa4\uff0c\u4e5f\u652f\u6301\u5df2\u6709\u96c6\u7fa4\u90e8\u7f72\uff0c\u8fd9\u91cc\u65b0\u5efaACK\u96c6\u7fa4\u90e8\u7f72, \u586b\u5165\u65b0\u5efaACK\u96c6\u7fa4\u6240\u9700\u7684\u53c2\u6570\uff1a 2.\u7136\u540e\u8bbe\u7f6e\u5b58\u653eDAG\u6587\u4ef6\u7684Git\u4ed3\u5e93\u914d\u7f6e\uff0c\u672c\u670d\u52a1\u91c7\u7528Git\u4ed3\u5e93\u4fdd\u5b58DAG\u6587\u4ef6\u7684\u65b9\u5f0f\uff0c\u8fd9\u91cc\u652f\u6301\u516c\u5f00\u548c\u79c1\u6709 \u4e24\u79cd\u4ed3\u5e93\uff0c\u9009\u62e9\u79c1\u6709\u4ed3\u5e93\u65f6\uff0c\u9700\u8981\u751f\u6210accessToken\u5f53\u4f5c\u8bbf\u95ee\u51ed\u8bc1\u3002 3.\u8bbe\u7f6eairflow web\u767b\u5f55\u7684\u8d26\u53f7\u548c\u5bc6\u7801\u3002 4.\u8bbe\u7f6eACK\u96c6\u7fa4\u90e8\u7f72\u7684\u7f51\u7edc\u8bbe\u7f6e\uff0c\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u5df2\u6709\u7684Vpc\u548cVSwitch\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u65b0\u5efaVpc\u548cVSwitch\u3002 5.\u786e\u8ba4\u8ba2\u5355\u5b8c\u6210\u540e\u540c\u610f\u670d\u52a1\u534f\u8bae\u5e76\u70b9\u51fb \u7acb\u5373\u521b\u5efa \u8fdb\u5165\u90e8\u7f72\u9636\u6bb5\u3002 6.\u5728\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\u4e2d\u53ef\u4ee5\u770b\u5230\u670d\u52a1\u5b9e\u4f8b\u5177\u4f53\u90e8\u7f72\u8fdb\u5ea6\u3002 7.\u90e8\u7f72\u5b8c\u6210\u540e\u5728\u63a7\u5236\u53f0\u627e\u5230Airflow\u670d\u52a1\u94fe\u63a5\u5e76\u8bbf\u95ee, \u521d\u59cb\u8d26\u53f7\u3001\u5bc6\u7801\u4e3a\u521b\u5efa\u670d\u52a1\u5b9e\u4f8b\u8bbe\u7f6e\u7684\u503c\u3002","title":"\u90e8\u7f72\u6d41\u7a0b"},{"location":"#_5","text":"\u4e0a\u9762\u670d\u52a1\u5b9e\u4f8b\u5df2\u7ecf\u90e8\u7f72\u5b8c\u6210\uff0c\u90a3\u4e48\u600e\u4e48\u8fd0\u884c\u6211\u4eec\u5b9a\u4e49\u597d\u7684DAG\u5de5\u4f5c\u6d41\u5462\uff0c\u8fd9\u91cc\u4e3b\u8981\u4f9d\u8d56git\u4ed3\u5e93\u53bb\u505a\u540c\u6b65\uff0c \u6211\u4eec\u53ef\u4ee5\u628a\u5199\u597d\u7684DAG\u6587\u4ef6\u63d0\u4ea4\u5230\u521b\u5efa\u670d\u52a1\u5b9e\u4f8b\u65f6\u6240\u51c6\u5907\u7684git\u4ed3\u5e93\u4e2d\uff0c\u7136\u540eairflow-scheduler\u7ec4\u4ef6\u4f1a\u8fdb\u884c\u540c\u6b65\uff0c web\u4e0a\u5c31\u80fd\u770b\u5230\u6211\u4eec\u5b9a\u4e49\u597d\u7684DAG\u6587\u4ef6\uff0c\u7136\u540e\u70b9\u51fbrun\u6309\u94ae\u5c31\u53ef\u4ee5\u8fd0\u884cDAG\u6587\u4ef6\u4e86\u3002 \u4e0b\u9762\u4ee5\u4e00\u4e2a\u7b80\u5355\u7684DAG\u6587\u4ef6\u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u5728airflow\u4e2d\u8fdb\u884c\u8fd0\u884cDAG\u3002 1.\u5728git\u4ed3\u5e93\u4e2d\u521b\u5efaDAG\u6587\u4ef6\uff0c\u6587\u4ef6\u540d\u4e3a hello_world_dag.py \uff0c\u91cc\u9762\u6709\u4e09\u4e2a\u4efb\u52a1\uff0c\u4f1a\u4f9d\u6b21\u6267\u884c\uff1a - \u6253\u5370\"Hello\" - \u6253\u5370\"World\" - \u4f11\u7720300\u79d2 import time from datetime import timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.utils.dates import days_ago # \u5b9a\u4e49\u9ed8\u8ba4\u53c2\u6570 default_args = { 'owner': 'airflow', # DAG \u7684\u6240\u6709\u8005 'start_date': days_ago(1), # DAG \u7684\u5f00\u59cb\u65f6\u95f4\uff081 \u5929\u524d\uff09 'retries': 1, # \u4efb\u52a1\u5931\u8d25\u65f6\u7684\u91cd\u8bd5\u6b21\u6570 'retry_delay': timedelta(minutes=5), # \u91cd\u8bd5\u95f4\u9694 } # \u5b9a\u4e49 DAG \u5bf9\u8c61 with DAG( dag_id='hello_world_dag', # DAG \u7684\u552f\u4e00\u6807\u8bc6\u7b26 default_args=default_args, # \u4f7f\u7528\u9ed8\u8ba4\u53c2\u6570 schedule_interval='@daily', # \u6bcf\u5929\u8fd0\u884c\u4e00\u6b21 catchup=False, # \u662f\u5426\u8865\u8dd1\u5386\u53f2\u4efb\u52a1 ) as dag: # \u5b9a\u4e49\u7b2c\u4e00\u4e2a\u4efb\u52a1\uff1a\u6253\u5370 \"Hello\" def print_hello(): print(\"Hello\") task_hello = PythonOperator( task_id='print_hello', # \u4efb\u52a1\u7684\u552f\u4e00\u6807\u8bc6\u7b26 python_callable=print_hello, # \u8c03\u7528\u7684 Python \u51fd\u6570 ) # \u5b9a\u4e49\u7b2c\u4e8c\u4e2a\u4efb\u52a1\uff1a\u6253\u5370 \"World\" def print_world(): print(\"World\") task_world = PythonOperator( task_id='print_world', python_callable=print_world, ) # \u5b9a\u4e49\u4e00\u4e2a\u4f11\u7720\u4efb\u52a1 def sleep_task(): print(\"Task is sleeping for 300 seconds...\") time.sleep(300) # \u4f11\u7720 300 \u79d2 print(\"Task woke up!\") sleep_operator = PythonOperator( task_id='sleep_task', python_callable=sleep_task, ) # \u8bbe\u7f6e\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb task_hello >> task_world >> sleep_operator 2.\u63d0\u4ea4DAG\u6587\u4ef6\u5230git\u4ed3\u5e93\u4e2d\uff0c\u7136\u540e\u53bbweb\u7aef\u67e5\u770b\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684DAG\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4f1a\u6709\u5ef6\u65f6\uff0c\u76ee\u524d\u8bbe\u7f6e\u7684\u662f\u6bcf10s\u540c\u6b65\u4e00\u6b21\u3002 3.\u6267\u884c\u8fd9\u4e2aDAG\uff0c\u70b9\u51fbrun\u6309\u94ae\uff0c\u70b9\u51fb\u8fdb\u884c\uff0c\u53ef\u4ee5\u770b\u5230\u6267\u884c\u8bb0\u5f55\uff0c\u70b9\u51fbGraph, \u53ef\u4ee5\u770b\u5230\u5177\u4f53\u6267\u884c\u6b65\u9aa4\uff0c \u53ef\u4ee5\u770b\u5230print_hello\u548cprint_world\u90fd\u5df2\u7ecf\u6267\u884c\u5b8c\u4e86\uff0csleep_task\u8fd8\u5728\u6267\u884c\u4e2d\uff0c\u8fd9\u4e2a\u529f\u80fd\u786e\u5b9e\u5f88\u5f3a\u5927\u3002 4.\u70b9\u51fb\u8fd8\u5728\u6267\u884c\u4e2d\u7684sleep_task\uff0c\u53ef\u4ee5\u5728Logs\u91cc\u770b\u5230\u8f93\u51fa\u4fe1\u606f\uff0c\u91cc\u9762\u8f93\u51fa\u4e86\u4f1asleep 300\u79d2\uff0c\u53ef\u89c1\u5728\u6b63\u5e38\u6267\u884c\u3002 \u901a\u8fc7\u4e0a\u9762\u8fd9\u4e2a\u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u51faairflow\u6574\u4f53\u529f\u80fd\u8fd8\u662f\u5f88\u5f3a\u5927\u7684\uff0c\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230DAG\u7684\u6267\u884c\u60c5\u51b5\uff0c\u5e76\u4e14\u5c06\u6bcf\u4e00\u6b65\u7684 \u6267\u884c\u8fc7\u7a0b\u90fd\u4ee5\u56fe\u5f62\u5316\u7684\u65b9\u5f0f\u663e\u793a\u51fa\u6765\uff0c\u91cc\u9762\u8fd8\u6709\u6267\u884c\u65f6\u95f4\u548c\u65e5\u5fd7\uff0c\u7528\u6765\u505a\u5de5\u4f5c\u6d41\u8fd8\u662f\u5f88\u597d\u7528\u7684\u3002","title":"\u4f7f\u7528\u65b9\u5f0f"},{"location":"#_6","text":"1.git-sync\u5bb9\u5668\u542f\u52a8\u5931\u8d25 \u7531\u4e8e\u9700\u8981\u8bbf\u95eegit\u4ed3\u5e93\uff0c\u56fd\u5185\u673a\u5668\u53ef\u80fd\u65e0\u6cd5\u8bbf\u95eegithub\uff0c\u53ef\u4ee5\u9009\u62e9\u6d77\u5916\u5730\u57df\u8fdb\u884c\u90e8\u7f72\uff0c\u53ef\u4ee5\u81ea\u884c\u5bf9\u90e8\u7f72\u7684helm release\u8fdb\u884c\u5347\u7ea7\u4fee\u6539\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u5f0f\u53ef\u4ee5\u53c2\u8003 \u5b98\u65b9values.yaml\u914d\u7f6e \u3002","title":"\u5e38\u89c1\u95ee\u9898\u7b54\u7591"},{"location":"index-en/","text":"Quick Deployment of Airflow on ComputeNest Disclaimer: This service is provided by a third party. We strive to ensure its security, accuracy, and reliability but cannot guarantee it is entirely free from failures, interruptions, errors, or attacks. Therefore, our company hereby disclaims any representations, warranties, or guarantees regarding the content, accuracy, completeness, reliability, suitability, or timeliness of this service. We are not liable for any direct or indirect losses or damages caused by your use of this service. For third-party websites, applications, products, and services accessed through this service, we assume no responsibility for their content, accuracy, completeness, reliability, suitability, or timeliness. You bear all risks and responsibilities arising from the consequences of using this service. We are not liable for any losses or damages, including but not limited to direct losses, indirect losses, lost profits, damaged reputation, data loss, or other economic losses, even if we were previously informed of the possibility of such losses or damages. We reserve the right to modify this disclaimer at any time, so please check it regularly before using this service. If you have any questions or concerns about this disclaimer or this service, please contact us. Overview Apache Airflow is an open-source workflow management platform used for writing, scheduling, and monitoring workflows. It was initially developed by Airbnb and donated to the Apache Software Foundation in 2016. The core idea of Airflow is to define workflows through code, making workflow management and maintenance more flexible and scalable. For the GitHub community page, see link . Billing Information The cost of deploying Airflow primarily involves: Selected vCPU and memory specifications System disk type and capacity Public network bandwidth Kubernetes cluster specifications Deployment Architecture Required Permissions for RAM Accounts Deploying Airflow requires access and creation operations for certain Alibaba Cloud resources. Therefore, your account needs to include permissions for the following resources: Note : These permissions are required only if your account is a RAM account. Permission Policy Name Remarks AliyunECSFullAccess Full access to manage Elastic Compute Service (ECS) AliyunVPCFullAccess Full access to manage Virtual Private Cloud (VPC) AliyunROSFullAccess Full access to manage Resource Orchestration Service (ROS) AliyunComputeNestUserFullAccess User-side permissions for ComputeNest service AliyunPostGreSQLFullAccess Full access to manage PostgreSQL database services AliyunSLBFullAccess Full access to manage Server Load Balancer (SLB) Deployment Process Visit the Airflow service deployment link , and fill in the deployment parameters as prompted. First, set up the container cluster. You can choose to create a new container cluster or deploy to an existing one. Here, we will deploy a new ACK cluster and input the required parameters for creating the ACK cluster: Next, configure the Git repository that will store DAG files. This service uses a Git repository to store DAG files. Set the username and password for Airflow web login. Configure the network settings for the ACK cluster deployment. You can choose an existing VPC and VSwitch or create a new one. After confirming the order, agree to the service agreement and click Create Now to proceed to the deployment phase. In the service instance list, you can view the specific deployment progress of the service instance. After the deployment is complete, locate the Airflow service link in the console and access it. The initial username and password are the values set during service instance creation. How to Use Once the service instance has been deployed, how do we run our defined DAG workflows? This mainly relies on the Git repository for synchronization. We can commit the written DAG files to the Git repository prepared during the creation of the service instance. Then, the airflow-scheduler component will synchronize them. On the web interface, you will see the defined DAG files, and you can click the \"Run\" button to execute the DAG file. Below is an example of a simple DAG file to demonstrate how to run a DAG in Airflow. Create a DAG file in the Git repository named hello_world_dag.py . It contains three tasks that will be executed sequentially: Print \"Hello\" Print \"World\" Sleep for 300 seconds import time from datetime import timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.utils.dates import days_ago # Define default arguments default_args = { 'owner': 'airflow', # Owner of the DAG 'start_date': days_ago(1), # Start date of the DAG (1 day ago) 'retries': 1, # Number of retries upon task failure 'retry_delay': timedelta(minutes=5), # Retry interval } # Define the DAG object with DAG( dag_id='hello_world_dag', # Unique identifier for the DAG default_args=default_args, # Use default arguments schedule_interval='@daily', # Run once daily catchup=False, # Whether to backfill historical tasks ) as dag: # Define the first task: print \"Hello\" def print_hello(): print(\"Hello\") task_hello = PythonOperator( task_id='print_hello', # Unique identifier for the task python_callable=print_hello, # Callable Python function ) # Define the second task: print \"World\" def print_world(): print(\"World\") task_world = PythonOperator( task_id='print_world', python_callable=print_world, ) # Define a sleep task def sleep_task(): print(\"Task is sleeping for 300 seconds...\") time.sleep(300) # Sleep for 300 seconds print(\"Task woke up!\") sleep_operator = PythonOperator( task_id='sleep_task', python_callable=sleep_task, ) # Set task dependencies task_hello >> task_world >> sleep_operator Commit the DAG file to the Git repository, then check the web interface. You will see the corresponding DAG. Note that there may be a delay in synchronization, which is currently set to occur every 10 seconds. Execute the DAG by clicking the \"Run\" button. You can view the execution record, and by clicking \"Graph,\" you can see the detailed steps of execution. You will notice that print_hello and print_world have completed execution, while sleep_task is still running. This feature is indeed powerful. Click on the running sleep_task , and in the Logs section, you can see the output information, indicating that it will sleep for 300 seconds. This confirms normal execution. From the above example, it is clear that Airflow has robust functionality. You can clearly see the execution status of DAGs, with each step displayed graphically. Execution times and logs are also available, making it highly suitable for workflow management. FAQ Git-sync container startup failure Due to the need to access the Git repository, domestic machines may not be able to access GitHub. You can choose to deploy in overseas regions. Additionally, note that if the Git repository is private, you must provide an access key. You will need to manually upgrade and modify the deployed Helm release. For configuration details, refer to the official values.yaml configuration .","title":"Quick Deployment of Airflow on ComputeNest"},{"location":"index-en/#quick-deployment-of-airflow-on-computenest","text":"Disclaimer: This service is provided by a third party. We strive to ensure its security, accuracy, and reliability but cannot guarantee it is entirely free from failures, interruptions, errors, or attacks. Therefore, our company hereby disclaims any representations, warranties, or guarantees regarding the content, accuracy, completeness, reliability, suitability, or timeliness of this service. We are not liable for any direct or indirect losses or damages caused by your use of this service. For third-party websites, applications, products, and services accessed through this service, we assume no responsibility for their content, accuracy, completeness, reliability, suitability, or timeliness. You bear all risks and responsibilities arising from the consequences of using this service. We are not liable for any losses or damages, including but not limited to direct losses, indirect losses, lost profits, damaged reputation, data loss, or other economic losses, even if we were previously informed of the possibility of such losses or damages. We reserve the right to modify this disclaimer at any time, so please check it regularly before using this service. If you have any questions or concerns about this disclaimer or this service, please contact us.","title":"Quick Deployment of Airflow on ComputeNest"},{"location":"index-en/#overview","text":"Apache Airflow is an open-source workflow management platform used for writing, scheduling, and monitoring workflows. It was initially developed by Airbnb and donated to the Apache Software Foundation in 2016. The core idea of Airflow is to define workflows through code, making workflow management and maintenance more flexible and scalable. For the GitHub community page, see link .","title":"Overview"},{"location":"index-en/#billing-information","text":"The cost of deploying Airflow primarily involves: Selected vCPU and memory specifications System disk type and capacity Public network bandwidth Kubernetes cluster specifications","title":"Billing Information"},{"location":"index-en/#deployment-architecture","text":"","title":"Deployment Architecture"},{"location":"index-en/#required-permissions-for-ram-accounts","text":"Deploying Airflow requires access and creation operations for certain Alibaba Cloud resources. Therefore, your account needs to include permissions for the following resources: Note : These permissions are required only if your account is a RAM account. Permission Policy Name Remarks AliyunECSFullAccess Full access to manage Elastic Compute Service (ECS) AliyunVPCFullAccess Full access to manage Virtual Private Cloud (VPC) AliyunROSFullAccess Full access to manage Resource Orchestration Service (ROS) AliyunComputeNestUserFullAccess User-side permissions for ComputeNest service AliyunPostGreSQLFullAccess Full access to manage PostgreSQL database services AliyunSLBFullAccess Full access to manage Server Load Balancer (SLB)","title":"Required Permissions for RAM Accounts"},{"location":"index-en/#deployment-process","text":"Visit the Airflow service deployment link , and fill in the deployment parameters as prompted. First, set up the container cluster. You can choose to create a new container cluster or deploy to an existing one. Here, we will deploy a new ACK cluster and input the required parameters for creating the ACK cluster: Next, configure the Git repository that will store DAG files. This service uses a Git repository to store DAG files. Set the username and password for Airflow web login. Configure the network settings for the ACK cluster deployment. You can choose an existing VPC and VSwitch or create a new one. After confirming the order, agree to the service agreement and click Create Now to proceed to the deployment phase. In the service instance list, you can view the specific deployment progress of the service instance. After the deployment is complete, locate the Airflow service link in the console and access it. The initial username and password are the values set during service instance creation.","title":"Deployment Process"},{"location":"index-en/#how-to-use","text":"Once the service instance has been deployed, how do we run our defined DAG workflows? This mainly relies on the Git repository for synchronization. We can commit the written DAG files to the Git repository prepared during the creation of the service instance. Then, the airflow-scheduler component will synchronize them. On the web interface, you will see the defined DAG files, and you can click the \"Run\" button to execute the DAG file. Below is an example of a simple DAG file to demonstrate how to run a DAG in Airflow. Create a DAG file in the Git repository named hello_world_dag.py . It contains three tasks that will be executed sequentially: Print \"Hello\" Print \"World\" Sleep for 300 seconds import time from datetime import timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.utils.dates import days_ago # Define default arguments default_args = { 'owner': 'airflow', # Owner of the DAG 'start_date': days_ago(1), # Start date of the DAG (1 day ago) 'retries': 1, # Number of retries upon task failure 'retry_delay': timedelta(minutes=5), # Retry interval } # Define the DAG object with DAG( dag_id='hello_world_dag', # Unique identifier for the DAG default_args=default_args, # Use default arguments schedule_interval='@daily', # Run once daily catchup=False, # Whether to backfill historical tasks ) as dag: # Define the first task: print \"Hello\" def print_hello(): print(\"Hello\") task_hello = PythonOperator( task_id='print_hello', # Unique identifier for the task python_callable=print_hello, # Callable Python function ) # Define the second task: print \"World\" def print_world(): print(\"World\") task_world = PythonOperator( task_id='print_world', python_callable=print_world, ) # Define a sleep task def sleep_task(): print(\"Task is sleeping for 300 seconds...\") time.sleep(300) # Sleep for 300 seconds print(\"Task woke up!\") sleep_operator = PythonOperator( task_id='sleep_task', python_callable=sleep_task, ) # Set task dependencies task_hello >> task_world >> sleep_operator Commit the DAG file to the Git repository, then check the web interface. You will see the corresponding DAG. Note that there may be a delay in synchronization, which is currently set to occur every 10 seconds. Execute the DAG by clicking the \"Run\" button. You can view the execution record, and by clicking \"Graph,\" you can see the detailed steps of execution. You will notice that print_hello and print_world have completed execution, while sleep_task is still running. This feature is indeed powerful. Click on the running sleep_task , and in the Logs section, you can see the output information, indicating that it will sleep for 300 seconds. This confirms normal execution. From the above example, it is clear that Airflow has robust functionality. You can clearly see the execution status of DAGs, with each step displayed graphically. Execution times and logs are also available, making it highly suitable for workflow management.","title":"How to Use"},{"location":"index-en/#faq","text":"Git-sync container startup failure Due to the need to access the Git repository, domestic machines may not be able to access GitHub. You can choose to deploy in overseas regions. Additionally, note that if the Git repository is private, you must provide an access key. You will need to manually upgrade and modify the deployed Helm release. For configuration details, refer to the official values.yaml configuration .","title":"FAQ"}]}